<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Traffic Light Optimization with Reinforcement Learning</title>
    <link rel="stylesheet" href="project.css">
</head>
<body>
    <header>
        <h1>Traffic Light Optimization with Reinforcement Learning</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../contact/index.html">Contact</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="introduction"></section>
            <h2>Project Overview</h2>
            <p>
                This project uses reinforcement learning to optimize traffic light management at a single intersection. 
                By leveraging the traffic simulation software SUMO and reinforcement learning algorithms, the goal was to reduce vehicle congestion, 
                minimize waiting times, and improve overall traffic throughput within a simulated environment.
            </p>
            <h3>Python Libraries Used</h3>
            <p>
                The following Python libraries were used to implement and train the reinforcement learning model:
            </p>
            <ul>
                <li><strong>Stable Baselines3:</strong> Used for getting access to reinforcement learning models </li>
                <li><strong>sumo_rl:</strong> A library that integrates the SUMO traffic simulation with the Gymnasium framework, enabling the creation of a custom reinforcement learning environment.</li>
            </ul>

        </section>
        <section id="sumo-role">
            <h2>What is SUMO and Its Role in This Project?</h2>
            <p>
                SUMO (Simulation of Urban MObility) is an open-source traffic simulation software designed to model traffic scenarios and generate realistic traffic flows. 
                In this project, SUMO played a critical role in simulating a single traffic intersection, where vehicles, traffic lights, and their interactions were modeled in a realistic environment.
            </p>
            <p>
                The project utilized the <code>sumo_rl</code> library to integrate SUMO with the <code>Gymnasium</code> reinforcement learning framework, enabling the creation of a custom training environment. 
            </p>
        </section>
        <section id="method">
            <h2>Method Used</h2>
            <p>
                The agent controlling the traffic lights was trained using Deep Q-Learning (DQN), a reinforcement learning algorithm. 
            </p>
            <h2>Why Deep Q-Learning ?</h2>
            <li> A reinforcement learning agent built with a Deep Q-Network combines Q-learning and Deep neural networks to predict the expected return before making a decision, which enables the model to generalize well to unseen cases.</li>
            <h3> Metrics used to calculate the reward used for the training phase</h3>
            <ul>
                <li><strong>Lane density </strong>: The proportion of a lane occupied by vehicles.</li>
                <li><strong>Lane queue </strong>: The length of the queue of stationary vehicles in each lane.</li>
                <li><strong>Current traffic light state </strong>: Represented as a one-hot encoded vector.</li>
                <li><strong>Minimum green condition </strong>: A boolean ensuring traffic lights stay green for a minimum duration.</li>
            </ul>
            <h3>Ensuring the Best Model</h3>
            <p> 
                Several versions of the DQN model were trained, each fine-tuned with different hyperparameter values to achieve the best performance. 
                The evaluation focused on reducing cumulative vehicle waiting times while ensuring the agent‚Äôs policy was adaptable to varying traffic conditions. 
                By systematically testing and analyzing each model‚Äôs performance, the most stable and efficient version was selected as the final model.
            </p>
            <h3>Hyperparameters Tested</h3>
    <p>
        The following hyperparameters were tested and adjusted to ensure the best performance:
    </p>
    <ul>
        <li>
            <strong>Learning rate (Œ±):</strong> Controls how much the neural network weights are updated during training. 
        </li>
        <li>
            <strong>Gamma (ùõæ):</strong> Determines how much the agent values future rewards compared to immediate rewards. 
        </li>
        <li>
            <strong>Buffer size:</strong> The size of the replay buffer that stores past experiences for training. 
            Larger buffers provide more diverse training data, while smaller buffers focus on recent events.
        </li>
        <li>
            <strong>Target Update Interval:</strong> Specifies how frequently the target network‚Äôs weights are updated. 
        </li>
        <li>
            <strong>Initial and Final Epsilon Value:</strong> Controls the balance between exploration and exploitation. 
            A high initial epsilon encourages exploration at the start, while a lower final epsilon ensures the agent exploits the best-known policies later in training.
        </li>
        <li>
            <strong>Exploration Fraction:</strong> Determines the percentage of total training steps over which the exploration rate is reduced from its initial to final value. 
        </li>
        <h3>Code for Training the Model</h3>
    <p>
        Below is the Python code used to train the final DQN model:
    </p>
    <pre>
<code>model = DQN(
    "MlpPolicy",
    env,
    gamma=0.95,
    learning_rate=0.0002,
    buffer_size=100000,
    batch_size=32,
    target_update_interval=4000 / 5,  # Update target network every episode (agent acts every 5 simulation steps)
    exploration_initial_eps=0.9,
    exploration_final_eps=0.1, 
    exploration_fraction=0.1,
    tensorboard_log=logs_dir,
    verbose=1
)
</code>
    </pre>
    <p>
        The above code defines the DQN model chosen and its tuned hyperparameters. 
        The training process was logged using TensorBoard for performance visualization and the training process can be visualized via SUMO.
    </p>
    <div class="simulation-gif">
        <figure>
            <img src="static/simulation_footage.gif" alt="Traffic light optimization simulation footage">
            <figcaption>Figure: Simulation footage of the model in training.</figcaption>
        </figure>
    </div>
    </section>


    </ul>
        </section>
        <section id="results">
            <h3>Model Performance Evaluation</h3>
            <p>
                The final decision on the best model was made by analyzing performance metrics, such as mean reward and loss function per episode, across multiple trained models.
                TensorBoard was used to monitor these metrics during training and testing, providing valuable insights into the model's learning progress and stability.
                Promising results from the chosen model were further validated by comparing its performance to a conventional cyclic light control algorithm.
            </p>

            <h3>Insights from TensorBoard Metrics</h3>
            <p>
                The graph of the <strong>mean reward per episode</strong> (Figure 1) demonstrates that the agent initially explores a bigger range of actions,
                resulting in lower rewards. Over time, the agent learns an effective policy, as shown by the steady increase in rewards and subsequent stabilization.
                This indicates that the model converges to an optimized solution which can be tested after.
            </p>
            <p>
                The <strong>loss function results</strong> (Figure 2) show an initial decline followed by smaller fluctuations as the model converges. 
                This pattern confirms that the training process was stable, and the model was able to generalize effectively while minimizing overfitting.
            </p>
            <div class="tensorboard-graphs">
                <figure>
                    <img src="static/mean_reward.png" alt="Mean reward per episode">
                    <figcaption>Figure 1: Mean reward per episode. The model converges on a policy that maximizes traffic throughput.</figcaption>
                </figure>
                <figure>
                    <img src="static/loss_function.png" alt="Loss function results per episode">
                    <figcaption>Figure 2: Loss function results per episode. A steady decrease indicates stable and consistent learning.</figcaption>
                </figure>
            </div>

            <h3>Comparison with Conventional Methods</h3>
<p>
    To demonstrate the effectiveness of the reinforcement learning model, I compared its performance against a traditional cyclic traffic light control algorithm.
    The results, visualized in the figure below, highlight the significant advantages of the final chosen model over the conventional approach.
</p>
<div class="comparison-graph">
    <figure>
        <img src="static/accumulated_waiting_time.png" alt="Comparison of accumulated waiting times">
        <figcaption>Figure 3: Total accumulated waiting time of vehicles for the cyclic agent vs. the reinforcement learning agent (Tuned DQN V2).</figcaption>
    </figure>
</div>
<p>
    The graph shows that the DQN model, represented in red, consistently maintained a lower and more stable cumulative waiting time compared to the cyclic agent, represented in blue, across 100,000 simulation steps. 
</p>
<p>
    Furthermore, the DQN model showed adaptability in handling peak waiting times. Whenever a peak occurred, the model quickly adjusted its actions to bring the waiting time back down, ensuring smooth and efficient traffic flow. 
    In contrast, the cyclic agent reached a constant waiting time and lacked the capacity to adapt and reduce itw waiting time.
</p>
            
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Jonathan Roy-Ascanio. All rights reserved.</p>
    </footer>
</body>
</html>