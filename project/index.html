<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Traffic Light Optimization with Reinforcement Learning</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1>Traffic Light Optimization with Reinforcement Learning</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../about/index.html">About</a></li>
                <li><a href="../contact/index.html">Contact</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="introduction"></section>
            <h2>Project Overview</h2>
            <p>
                This project uses reinforcement learning to optimize traffic light management at a single intersection. 
                By leveraging the traffic simulation software SUMO and reinforcement learning algorithms, the goal was to reduce vehicle congestion, 
                minimize waiting times, and improve overall traffic throughput within a simulated environment.
            </p>
            <h3>Python Libraries Used</h3>
            <p>
                The following Python libraries were used to implement and train the reinforcement learning model:
            </p>
            <ul>
                <li><strong>Stable Baselines3:</strong> Used for getting access to reinforcement learning models </li>
                <li><strong>sumo_rl:</strong> A library that integrates the SUMO traffic simulation with the Gymnasium framework, enabling the creation of a custom reinforcement learning environment.</li>
            </ul>

        </section>
        <section id="sumo-role">
            <h2>What is SUMO and Its Role in This Project?</h2>
            <p>
                SUMO (Simulation of Urban MObility) is an open-source traffic simulation software designed to model traffic scenarios and generate realistic traffic flows. 
                In this project, SUMO played a critical role in simulating a single traffic intersection, where vehicles, traffic lights, and their interactions were modeled in a realistic environment.
            </p>
            <p>
                The project utilized the <code>sumo_rl</code> library to integrate SUMO with the <code>Gymnasium</code> reinforcement learning framework, enabling the creation of a custom training environment. 
            </p>
        </section>
        <section id="method">
            <h2>Method Used</h2>
            <p>
                The agent controlling the traffic lights was trained using Deep Q-Learning (DQN), a reinforcement learning algorithm. 
            </p>
            <h2>Why Deep Q-Learning ?</h2>
            <li> A reinforcement learning agent built with a Deep Q-Network combines Q-learning and Deep neural networks to predict the expected return before making a decision, which enables the model to generalize well to unseen cases.</li>
            <h3> Metrics used to calculate the reward used for the training phase</h3>
            <ul>
                <li><strong>Lane density </strong>: The proportion of a lane occupied by vehicles.</li>
                <li><strong>Lane queue </strong>: The length of the queue of stationary vehicles in each lane.</li>
                <li><strong>Current traffic light state </strong>: Represented as a one-hot encoded vector.</li>
                <li><strong>Minimum green condition </strong>: A boolean ensuring traffic lights stay green for a minimum duration.</li>
            </ul>
            <h3>Ensuring the Best Model</h3>
            <p> 
                Several versions of the DQN model were trained, each fine-tuned with different hyperparameter values to achieve the best performance. 
                The evaluation focused on reducing cumulative vehicle waiting times while ensuring the agent‚Äôs policy was adaptable to varying traffic conditions. 
                By systematically testing and analyzing each model‚Äôs performance, the most stable and efficient version was selected as the final model.
            </p>
            <h3>Hyperparameters Tested</h3>
    <p>
        The following hyperparameters were tested and adjusted to ensure the best performance:
    </p>
    <ul>
        <li>
            <strong>Learning rate (Œ±):</strong> Controls how much the neural network weights are updated during training. 
        </li>
        <li>
            <strong>Gamma (ùõæ):</strong> Determines how much the agent values future rewards compared to immediate rewards. 
        </li>
        <li>
            <strong>Buffer size:</strong> The size of the replay buffer that stores past experiences for training. 
            Larger buffers provide more diverse training data, while smaller buffers focus on recent events.
        </li>
        <li>
            <strong>Target Update Interval:</strong> Specifies how frequently the target network‚Äôs weights are updated. 
        </li>
        <li>
            <strong>Initial and Final Epsilon Value:</strong> Controls the balance between exploration and exploitation. 
            A high initial epsilon encourages exploration at the start, while a lower final epsilon ensures the agent exploits the best-known policies later in training.
        </li>
        <li>
            <strong>Exploration Fraction:</strong> Determines the percentage of total training steps over which the exploration rate is reduced from its initial to final value. 
        </li>
        <h3>Code for Training the Model</h3>
    <p>
        Below is the Python code used to train the final DQN model:
    </p>
    <pre>
<code>model = DQN(
    "MlpPolicy",
    env,
    gamma=0.95,
    learning_rate=0.0002,
    buffer_size=100000,
    batch_size=32,
    target_update_interval=4000 / 5,  # Update target network every episode (agent acts every 5 simulation steps)
    exploration_initial_eps=0.9,
    exploration_final_eps=0.1, 
    exploration_fraction=0.1,
    tensorboard_log=logs_dir,
    verbose=1
)
    </pre>
    <p>
        The above code defines the DQN model chosen and its tuned hyperparameters. 
        The training process was logged using TensorBoard for performance visualization and the training process can be visualized via SUMO.
    </p>
    <div class="simulation-gif">
        <figure>
            <img src="static/simulation_footage.gif" alt="Traffic light optimization simulation footage">
            <figcaption>Figure: Simulation footage of the model in training.</figcaption>
        </figure>
    </div>
    </section>


    </ul>
        </section>
        <section id="results">
            <h3>Model Performance Evaluation</h3>
            <p>
                The final decision on the best model was made by analyzing performance metrics, such as mean reward and loss function, across multiple trained models.
                TensorBoard was used to monitor these metrics during training, providing valuable insights into the model's learning progress and stability.
                Promising results from the chosen model were further validated by comparing its performance to a conventional cyclic light control algorithm.
            </p>
            <div class="tensorboard-graphs">
                <figure>
                    <img src="static/mean_reward.png" alt="Mean reward per episode">
                    <figcaption>Figure 1: Mean reward per episode. The model converges on a policy that maximizes traffic throughput.</figcaption>
                </figure>
                <figure>
                    <img src="static/loss_function.png" alt="Loss function results per episode">
                    <figcaption>Figure 2: Loss function results per episode. A steady decrease indicates stable and consistent learning.</figcaption>
                </figure>
            </div>
            
        </section>
    </main>
    <footer>
        <p>&copy; 2024 [Your Name]. All rights reserved.</p>
    </footer>
</body>
</html>